{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4987d4-c66b-495e-9135-8a0d9b258c7e",
   "metadata": {},
   "source": [
    "## Kundyz Muktar - Lab 1 - Applied Machine Learning - February 16, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4a9b0c7-dff5-43f4-b4f8-9a60e677c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2):\n",
    "\n",
    "    if test_size <= 0 or test_size >= 1:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n",
    "\n",
    "    n = len(X)\n",
    "    i = int(n * (1 - test_size))\n",
    "    \n",
    "    X_train, y_train = X[:i], y[:i]\n",
    "    X_test, y_test = X[i:], y[i:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c96cd-9e81-437c-a65d-258d8378f8da",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec29d92b-16e1-4e3d-b691-d43ecbdac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fa410-8024-4d89-ab98-4c8f4d4e36da",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64e03856-669d-4267-802c-309ce60ef17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avoid printing out warnings## Avoid printing out warnings\n",
    "with warnings.catch_warnings():\n",
    "     warnings.filterwarnings(\"ignore\")\n",
    "     X, y = load_boston(return_X_y=True)\n",
    "with warnings.catch_warnings():\n",
    "     warnings.filterwarnings(\"ignore\")\n",
    "     X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_with_bias = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "X_b, x_test, y, y_test = train_test_split(X_with_bias, y, test_size=0.1)\n",
    "\n",
    "feature_means = X_b[:, 1:].mean(axis=0)\n",
    "feature_stds = X_b[:, 1:].std(axis=0)\n",
    "\n",
    "X_b[:, 1:] = (X_b[:, 1:] - feature_means) / feature_stds\n",
    "x_test[:, 1:] = (x_test[:, 1:] - feature_means) / feature_stds\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b88052-de0c-4576-9a8f-09e271a30a4f",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6acb9304-ce00-4176-911c-61ef337776f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE training: 22.93840526238035\n",
      "Mean MSE validation: 25.978551123247335\n",
      "MSE test: 10.80620138107843\n"
     ]
    }
   ],
   "source": [
    "mse_train3 = []\n",
    "mse_validation3 = []\n",
    "for train, validation in kf.split(X_b):\n",
    "    X_training, Y_training = X_b[train], y[train]\n",
    "    X_validation, Y_validation  = X_b[validation], y[validation]\n",
    "\n",
    "    weight_vector = np.linalg.inv(X_training.T @ X_training) @ X_training.T @ Y_training\n",
    "    \n",
    "    mse_train3.append(np.mean((Y_training - (X_training @ weight_vector)) ** 2))\n",
    "    mse_validation3.append(np.mean((Y_validation - (X_validation @ weight_vector)) ** 2))\n",
    "\n",
    "weight_vector_final = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_train_val\n",
    "mse_test3 = np.mean((y_test - (x_test @ weight_vector_final))**2)\n",
    "\n",
    "print(\"Mean MSE training:\", np.mean(mse_train3))\n",
    "print(\"Mean MSE validation:\", np.mean(mse_validation3))\n",
    "print(\"MSE test:\", mse_test3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76deb10d-b640-459f-8124-a35df1238ec5",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "887a5c2c-fda7-4f2a-a125-35690dec77a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best lambda is  10.0\n"
     ]
    }
   ],
   "source": [
    "mse_train4 = []\n",
    "mse_validation4 = []\n",
    "lambdas = np.logspace(1, 7, num=13)\n",
    "I = np.eye(X_b.shape[1])\n",
    "I[0,0]=0\n",
    "\n",
    "for lmbda in lambdas: \n",
    "    mse_train_per_lambda1 = []\n",
    "    mse_validation_per_lambda1 = []\n",
    "\n",
    "    for train, validation in kf.split(X_b):\n",
    "        X_training, Y_training = X_b[train], y[train]\n",
    "        X_validation, Y_validation  = X_b[validation], y[validation]\n",
    "    \n",
    "        weight_vector = np.linalg.inv(X_training.T @ X_training + lmbda * I) @ X_training.T @ Y_training\n",
    "        \n",
    "        Yh_training = X_training @ weight_vector\n",
    "        R1 = Y_training - Yh_training\n",
    "        loss1 = (R1.T @ R1) / len(Y_training)\n",
    "        mse_train_per_lambda1.append(loss1)\n",
    "            \n",
    "        Yh_validation = X_validation @ weight_vector\n",
    "        R2 = Y_validation - Yh_validation\n",
    "        loss2 = (R2.T @ R2) / len(Y_validation)\n",
    "        mse_validation_per_lambda1.append(loss2)\n",
    "        \n",
    "    mse_train4.append(np.mean(mse_train_per_lambda1)) \n",
    "    mse_validation4.append(np.mean(mse_validation_per_lambda1)) \n",
    "\n",
    "best_lambda_index = np.argmin(mse_validation4)\n",
    "\n",
    "# print(\"mse for training set:\", mse_train4)\n",
    "# print(\"mse for validation set:\", mse_validation4)\n",
    "print(\"the best lambda is \", lambdas[best_lambda_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfef86-42f6-4f9f-96df-95c712859c8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Task 4** The best choice of $\\lambda$ is $10^1$, for which we obtain minimum MSE across MSE varying depending on $\\lambda_i$. \n",
    "\n",
    "**Task 5** When $\\lambda$ = $10^1$, we have average (across K-folds) MSE for training set of score 21.87 and for validation set 23.47. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ad7cd-b9f0-49f6-bfaf-6b44052c36fd",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0efac25b-757f-4971-913b-6a9523d11f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE training: 23.091344595493844\n",
      "Mean MSE validation: 25.961663903882958\n",
      "MSE test: 9.728459652108022\n"
     ]
    }
   ],
   "source": [
    "mse_train5 = []\n",
    "mse_validation5 = []\n",
    "\n",
    "I = np.eye(X_b.shape[1])\n",
    "I[0,0]=0\n",
    "\n",
    "for train, validation in kf.split(X_b):\n",
    "    X_training, Y_training = X_b[train], y[train]\n",
    "    X_validation, Y_validation  = X_b[validation], y[validation]\n",
    "    \n",
    "    weight_vector = np.linalg.inv(X_training.T @ X_training + lambdas[best_lambda_index] * I) @ X_training.T @ Y_training\n",
    "    \n",
    "    mse_train5.append(np.mean((Y_training - (X_training @ weight_vector)) ** 2))\n",
    "    mse_validation5.append(np.mean((Y_validation - (X_validation @ weight_vector)) ** 2))\n",
    "\n",
    "weight_vector_final5 = np.linalg.inv(X_b.T @ X_b + lambdas[best_lambda_index] * I) @ X_b.T @ y_train_val\n",
    "mse_test5 = np.mean((y_test - (x_test @ weight_vector_final5)) ** 2)\n",
    "\n",
    "print(\"Mean MSE training:\", np.mean(mse_train5))\n",
    "print(\"Mean MSE validation:\", np.mean(mse_validation5))\n",
    "print(\"MSE test:\", mse_test5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c9a1e-f4ab-49eb-bce3-74238a07d8d7",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "061faa27-79a8-46ae-a743-b5d4ab341550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best lambda is  10.0\n"
     ]
    }
   ],
   "source": [
    "p = PolynomialFeatures(degree=2)\n",
    "X_p = p.fit_transform(X_b)\n",
    "X_p = np.concatenate([np.ones((X_p.shape[0], 1)), X_p], axis=1)\n",
    "\n",
    "# same as before, before this step i need to make sure that X_deg2 is of a type numpy array\n",
    "mse_train6 = []\n",
    "mse_validation6 = []\n",
    "lambdas = np.logspace(1, 7, num=13)\n",
    "I = np.eye(X_p.shape[1])\n",
    "I[0,0]=0\n",
    "\n",
    "for lmbda in lambdas: \n",
    "    mse_train_per_lambda2 = []\n",
    "    mse_validation_per_lambda2 = []\n",
    "\n",
    "    for train, validation in kf.split(X_p):\n",
    "        X_training, Y_training = X_p[train], y[train]\n",
    "        X_validation, Y_validation  = X_p[validation], y[validation]\n",
    "    \n",
    "        weight_vector = np.linalg.inv(X_training.T @ X_training + lmbda * I) @ X_training.T @ Y_training\n",
    "\n",
    "        Yh_training = X_training @ weight_vector\n",
    "        R1 = Y_training - Yh_training\n",
    "        loss1 = (R1.T @ R1) / len(Y_training)\n",
    "        mse_train_per_lambda2.append(loss1)\n",
    "          \n",
    "        Yh_validation = X_validation @ weight_vector\n",
    "        R2 = Y_validation - Yh_validation\n",
    "        loss2 = (R2.T @ R2) / len(Y_validation)\n",
    "        mse_validation_per_lambda2.append(loss2)\n",
    "        \n",
    "    mse_train6.append(np.mean(mse_train_per_lambda2)) \n",
    "    mse_validation6.append(np.mean(mse_validation_per_lambda2)) \n",
    "\n",
    "best_lambda_index = np.argmin(mse_validation6)\n",
    "\n",
    "# print(\"mse for training set:\", mse_train)\n",
    "# print(\"mse for validation set:\", mse_validation)\n",
    "print(\"the best lambda is \", lambdas[best_lambda_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead06d44-7627-461b-ab48-1b63613b7c0a",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7db952ee-580d-403c-ae73-ace893c091e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE training: 35.090146352768315\n",
      "Mean MSE validation: 39.23650317320514\n",
      "MSE test: 19.90529355507329\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "mse_train7 =[]\n",
    "mse_validation7 =[]\n",
    "for train, validation in kf.split(X_b):\n",
    "    X_training, Y_training = X_b[train], y[train]\n",
    "    X_validation, Y_validation = X_b[validation], y[validation]\n",
    "    \n",
    "    weight_vector = np.zeros(X_training.shape[1])\n",
    "    \n",
    "    for iteration in range(1000):\n",
    "\n",
    "        Yh_training = X_training @ weight_vector\n",
    "        n = X_training.shape[0]\n",
    "        \n",
    "        gradients = -2*(X_training.T @ (Y_training - Yh_training)/n)\n",
    "        \n",
    "        weight_vector = weight_vector - 0.001 * gradients\n",
    "        \n",
    "    mse_train7.append(np.mean((Y_training - (X_training @ weight_vector)) ** 2))\n",
    "    mse_validation7.append(np.mean((Y_validation - (X_validation @ weight_vector)) ** 2))\n",
    "\n",
    "mse_test7 = np.mean((y_test - (x_test @ weight_vector)) ** 2)\n",
    "\n",
    "print(\"Mean MSE training:\", np.mean(mse_train7))\n",
    "print(\"Mean MSE validation:\", np.mean(mse_validation7))\n",
    "print(\"MSE test:\", mse_test7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfdbd9-8f5d-467c-8812-7be238ca7fec",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "feaf3192-88e8-463b-b6f6-09e7a24a4231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE training: 35.09171127507219\n",
      "Mean MSE validation: 39.23747384724328\n",
      "MSE test: 19.902658689182118\n"
     ]
    }
   ],
   "source": [
    "mse_train8 =[]\n",
    "mse_validation8 =[]\n",
    "# Parameters\n",
    "lmbda = 0.001\n",
    "learning_rate = 0.001\n",
    "\n",
    "for train, validation in kf.split(X_b):\n",
    "    X_training, Y_training = X_b[train], y[train]\n",
    "    X_validation, Y_validation = X_b[validation], y[validation]\n",
    "    \n",
    "    weight_vector = np.zeros(X_training.shape[1])\n",
    "\n",
    "    for iteration in range(1000):  # You can change the iterations if needed\n",
    "\n",
    "        Yh_training = X_training @ weight_vector\n",
    "        n = X_training.shape[0]\n",
    "        \n",
    "        # Partial derivatives with respect to parameters of Lasso \n",
    "        pd_wrt_parameters = -2/n * (X_training.T @ (Y_training - Yh_training)) + lmbda * np.sign(weight_vector)\n",
    "        # regularize all weigths (except from the intercept) \n",
    "        weight_vector[1:] = weight_vector[1:] - learning_rate * pd_wrt_parameters[1:]\n",
    "        # gradient descent for the intercept\n",
    "        weight_vector[0] = weight_vector[0] - learning_rate * (-2/n) * np.sum(Y_training - Yh_training)\n",
    "        \n",
    "    mse_train8.append(np.mean((Y_training - (X_training @ weight_vector)) ** 2))\n",
    "    mse_validation8.append(np.mean((Y_validation - (X_validation @ weight_vector)) ** 2))\n",
    "\n",
    "mse_test8 = np.mean((y_test - (x_test @ weight_vector)) ** 2)\n",
    "\n",
    "print(\"Mean MSE training:\", np.mean(mse_train8))\n",
    "print(\"Mean MSE validation:\", np.mean(mse_validation8))\n",
    "print(\"MSE test:\", mse_test8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c823d89-2f7e-4ead-bd62-e427a93009f5",
   "metadata": {},
   "source": [
    "# Task 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6064c05c-f609-4e77-ab81-09fb11d7dcab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE training: 34.25833010084479\n",
      "Mean MSE validation: 34.25833010084479\n",
      "MSE test: 19.93877685674913\n"
     ]
    }
   ],
   "source": [
    "lmbda = 0.001\n",
    "learning_rate = 0.001\n",
    "\n",
    "def elastic_gradient(X, y, y_pred, theta):\n",
    "    r1 = 0.2  # L1 \n",
    "    r2 = 1-r1  # L2 \n",
    "    return (\n",
    "        -(2 / y.size) * X.T.dot(y - y_pred)  # Gradient of MSE\n",
    "        + lmbda * r2 * theta  # Gradient of L2 penalty\n",
    "        + lmbda * r1 * np.sign(theta)  # Subgradient of L1 penalty\n",
    "    )\n",
    "\n",
    "mse_train9 =[]\n",
    "mse_validation9 =[]\n",
    "\n",
    "for train, validation in kf.split(X_b):\n",
    "    X_training, Y_training = X_b[train], y[train]\n",
    "    X_validation, Y_validation = X_b[validation], y[validation]\n",
    "    \n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    for i in range(1000):\n",
    "        Yh_train = X_train @ theta\n",
    "        gradients = elastic_gradient(X_train, y_train, Yh_train, theta)\n",
    "        theta -= learning_rate * gradients\n",
    "        \n",
    "    mse_train9.append(np.mean((Y_training - (X_training @ weight_vector)) ** 2))\n",
    "    mse_validation9.append(np.mean((Y_validation - (X_validation @ weight_vector)) ** 2))\n",
    "\n",
    "mse_test9 = np.mean((y_test - (x_test @ theta)) ** 2)\n",
    "\n",
    "print(\"Mean MSE training:\", np.mean(mse_train9))\n",
    "print(\"Mean MSE validation:\", np.mean(mse_validation9))\n",
    "print(\"MSE test:\", mse_test9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25b52b-da9c-4492-a08b-f37f3e3c1fd6",
   "metadata": {},
   "source": [
    "# Task 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a189d9d-7f3a-446c-adf0-83bad5a8678a",
   "metadata": {},
   "source": [
    "From the given three models, I would choose Elastic Net.\n",
    "\n",
    "Firstly, while the test MSEs for three models are almost the same, in terms of training and MSEs\n",
    "\n",
    "In terms of the use cases of these three models, multivariate regression can overfit the data as it doesn't have regularization, and lasso regression eliminates features with the least impact on the prediction. Elastic Net has both L1 and L2 regularizations, which implies that it eliminates irrelevant features while also handling multicollinearity. Considering the features of the Boston dataset, it has only 13 features, so eliminating features might not be the best decision. However, looking at these features, one may say that some of them can be correlated. Elastic Net is very rebust, as it has both L1 and L2 regularizations, which implies that it eliminates irrelevant features while also handling multicollinearity.\n",
    "\n",
    "Elastic Net has two main parameters:\n",
    "\n",
    "Î±: the overall regularization strength.\n",
    "r: the ratio that balances the amount of L1 and L2 regularization.\n",
    "Additionally, since we are optimizing the Elastic Net model with Gradient Descent, the learning rate is also a parameter here, which determines the magnitude of the steps we are taking when updating the weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
